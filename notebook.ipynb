{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import imgaug.augmenters as iaa\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Reshape, GlobalAveragePooling2D\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import load_model \n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Dataset Path\n",
    "images_base_directory = \"../Captcha_solver/final_images/\"\n",
    "# images_base_directory = \"./my_generated_images\"\n",
    "# images_base_directory = \"./final_images\"\n",
    "# Function to get and shuffle image files\n",
    "def get_and_shuffle_image_files(directory):\n",
    "    image_files = os.listdir(directory)\n",
    "    random.shuffle(image_files)\n",
    "    return image_files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to get images names because ismein labels hain\n",
    "\n",
    "def load_data(image_files):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for file in image_files:\n",
    "        img = load_img(images_base_directory + \"/\" +  file, color_mode='grayscale')\n",
    "        img = img_to_array(img) / 255.0\n",
    "        label = os.path.basename(file).split('.')[0]\n",
    "        images.append(img)\n",
    "        labels.append(label)\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "def get_image_files(directory):\n",
    "    return os.listdir(directory)\n",
    "\n",
    "image_files = get_and_shuffle_image_files(images_base_directory)\n",
    "image_files = get_image_files(images_base_directory)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 80, 190, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(load_data(image_files[0:5])[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### processing labels and creating categorical label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only loading 80% of data for training\n",
    "images, labels = load_data(image_files)\n",
    "\n",
    "# Preprocess the labels\n",
    "\n",
    "def preprocess_labels(labels, num_characters, num_classes):\n",
    "    processed_labels = []\n",
    "    for label in labels:\n",
    "        # converting individual to int because kaam aayenge\n",
    "        label = [int(char) for char in label]\n",
    "        # Convert each integer to categorical\n",
    "        categorical_label = np.zeros((num_characters, num_classes))\n",
    "        for i, char in enumerate(label):\n",
    "            categorical_label[i, char] = 1\n",
    "        processed_labels.append(categorical_label)\n",
    "    return np.array(processed_labels)\n",
    "\n",
    "\n",
    "\n",
    "num_characters = 6  # no of chars\n",
    "num_classes = 10  # Digits 0-9\n",
    "labels = preprocess_labels(labels, num_characters, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 598ms/step - accuracy: 0.1395 - loss: 2.1902 - val_accuracy: 0.2744 - val_loss: 2.0215\n",
      "Epoch 2/30\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 593ms/step - accuracy: 0.6478 - loss: 0.9707 - val_accuracy: 0.2913 - val_loss: 1.8650\n",
      "Epoch 3/30\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 582ms/step - accuracy: 0.8167 - loss: 0.5061 - val_accuracy: 0.2995 - val_loss: 1.8601\n",
      "Epoch 4/30\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 589ms/step - accuracy: 0.8662 - loss: 0.3867 - val_accuracy: 0.3202 - val_loss: 1.8581\n",
      "Epoch 5/30\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 602ms/step - accuracy: 0.8790 - loss: 0.3453 - val_accuracy: 0.3533 - val_loss: 1.7694\n",
      "Epoch 6/30\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 572ms/step - accuracy: 0.8884 - loss: 0.3195 - val_accuracy: 0.3703 - val_loss: 1.7525\n",
      "Epoch 7/30\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 576ms/step - accuracy: 0.8944 - loss: 0.2987 - val_accuracy: 0.4137 - val_loss: 1.6912\n",
      "Epoch 8/30\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 572ms/step - accuracy: 0.9128 - loss: 0.2526 - val_accuracy: 0.4283 - val_loss: 1.6223\n",
      "Epoch 9/30\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 583ms/step - accuracy: 0.9183 - loss: 0.2350 - val_accuracy: 0.4381 - val_loss: 1.6244\n",
      "Epoch 10/30\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 585ms/step - accuracy: 0.9228 - loss: 0.2252 - val_accuracy: 0.4791 - val_loss: 1.5440\n",
      "Epoch 11/30\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 582ms/step - accuracy: 0.9295 - loss: 0.2083 - val_accuracy: 0.5178 - val_loss: 1.4370\n",
      "Epoch 12/30\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 607ms/step - accuracy: 0.9416 - loss: 0.1731 - val_accuracy: 0.5328 - val_loss: 1.4405\n",
      "Epoch 13/30\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 602ms/step - accuracy: 0.9404 - loss: 0.1738 - val_accuracy: 0.5638 - val_loss: 1.3682\n",
      "Epoch 14/30\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 588ms/step - accuracy: 0.9476 - loss: 0.1533 - val_accuracy: 0.5791 - val_loss: 1.2988\n",
      "Epoch 15/30\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 593ms/step - accuracy: 0.9517 - loss: 0.1445 - val_accuracy: 0.5836 - val_loss: 1.3159\n",
      "Epoch 16/30\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 588ms/step - accuracy: 0.9577 - loss: 0.1281 - val_accuracy: 0.6281 - val_loss: 1.1628\n",
      "Epoch 17/30\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 563ms/step - accuracy: 0.9598 - loss: 0.1172 - val_accuracy: 0.6334 - val_loss: 1.1565\n",
      "Epoch 18/30\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 608ms/step - accuracy: 0.9622 - loss: 0.1158 - val_accuracy: 0.6373 - val_loss: 1.1802\n",
      "Epoch 19/30\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 577ms/step - accuracy: 0.9643 - loss: 0.1033 - val_accuracy: 0.6608 - val_loss: 1.0842\n",
      "Epoch 20/30\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 480ms/step - accuracy: 0.9679 - loss: 0.0928 - val_accuracy: 0.6610 - val_loss: 1.0968\n",
      "Epoch 21/30\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 552ms/step - accuracy: 0.9693 - loss: 0.0918 - val_accuracy: 0.6719 - val_loss: 1.0755\n",
      "Epoch 22/30\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 582ms/step - accuracy: 0.9725 - loss: 0.0783 - val_accuracy: 0.6820 - val_loss: 1.0202\n",
      "Epoch 23/30\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 577ms/step - accuracy: 0.9733 - loss: 0.0768 - val_accuracy: 0.6902 - val_loss: 0.9984\n",
      "Epoch 24/30\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 584ms/step - accuracy: 0.9744 - loss: 0.0732 - val_accuracy: 0.6951 - val_loss: 0.9984\n",
      "Epoch 25/30\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 578ms/step - accuracy: 0.9762 - loss: 0.0689 - val_accuracy: 0.6890 - val_loss: 1.0447\n",
      "Epoch 26/30\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 578ms/step - accuracy: 0.9770 - loss: 0.0653 - val_accuracy: 0.7150 - val_loss: 0.9639\n",
      "Epoch 27/30\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 585ms/step - accuracy: 0.9775 - loss: 0.0660 - val_accuracy: 0.7189 - val_loss: 0.9622\n",
      "Epoch 28/30\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 578ms/step - accuracy: 0.9813 - loss: 0.0582 - val_accuracy: 0.7067 - val_loss: 0.9488\n",
      "Epoch 29/30\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 585ms/step - accuracy: 0.9797 - loss: 0.0599 - val_accuracy: 0.7185 - val_loss: 0.9710\n",
      "Epoch 30/30\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 576ms/step - accuracy: 0.9794 - loss: 0.0618 - val_accuracy: 0.7277 - val_loss: 0.9884\n",
      "(80, 190, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def create_model(input_shape, num_classes, num_characters):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "        \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(num_characters * num_classes, activation='softmax'))\n",
    "    model.add(Reshape((num_characters, num_classes)))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "input_shape = images[0].shape\n",
    "model = create_model(input_shape, num_classes, num_characters)\n",
    "\n",
    "model.fit(images, labels, epochs=30, batch_size=32, validation_split=0.2)\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "[[1.2712940e-13 8.8602197e-01 2.2274328e-08 1.5185835e-08 1.0991174e-11\n",
      "  4.3842711e-08 2.8127209e-09 1.0896701e-07 5.4500284e-09 7.8799488e-08]\n",
      " [7.3605712e-08 8.4312029e-02 1.3376275e-11 4.4702356e-10 6.6629504e-08\n",
      "  1.5768067e-09 1.4245451e-09 9.3182146e-09 2.0670605e-08 1.2598446e-10]\n",
      " [1.3622474e-07 1.8033555e-08 4.1180188e-09 9.5192924e-08 8.3348982e-07\n",
      "  4.1913918e-08 1.2254922e-02 4.6327076e-09 6.7923412e-07 3.3107330e-07]\n",
      " [6.5852657e-07 2.3315842e-08 6.0949624e-08 1.5091697e-05 5.8260905e-08\n",
      "  1.3695558e-06 1.2245989e-05 1.4899529e-08 6.6617446e-04 4.9733680e-06]\n",
      " [9.2611799e-07 4.8204252e-05 8.4520734e-06 9.3068923e-07 2.1465896e-03\n",
      "  5.7105458e-09 3.4053394e-06 3.4801055e-06 8.1223422e-10 2.2063136e-07]\n",
      " [1.4461931e-02 9.2453696e-08 9.8675457e-08 4.0152281e-07 1.9132843e-07\n",
      "  2.1545045e-08 9.6897350e-07 9.6562438e-08 7.3686027e-07 3.1091378e-05]]\n",
      "Predicted CAPTCHA: 116840\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Function to resize and normalize an image\n",
    "def preprocess_image(image_file, target_size):\n",
    "    img = cv2.imread(image_file, cv2.IMREAD_GRAYSCALE)  # Load the image in grayscale\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Image {image_file} not found.\")\n",
    "    img = cv2.resize(img, target_size) \n",
    "    img = img / 255.0  \n",
    "    # adding a channel dimension to the last \n",
    "    img = np.expand_dims(img, axis=-1) \n",
    "    img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "    return img\n",
    "\n",
    "# Function to predict the CAPTCHA\n",
    "def predict_captcha(model, preprocessed_img):\n",
    "    prediction = model.predict(preprocessed_img)\n",
    "    print(prediction[0])\n",
    "    predicted_label = ''.join([str(np.argmax(char)) for char in prediction[0]])\n",
    "    return predicted_label\n",
    "\n",
    "# Predict a new CAPTCHA image\n",
    "image_file = images_base_directory + '/' + \"116840.png\"\n",
    "preprocessed_img = preprocess_image(image_file, (190, 80))  # Resize to 190x80\n",
    "captcha = predict_captcha(model, preprocessed_img)\n",
    "print(f'Predicted CAPTCHA: {captcha}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
